Module llmreflect.LLMCore.LLMCore
=================================

Functions
---------

    
`Llama2Cpp(*args, **kwargs)`
:   

    
`in_workflow()`
:   

    
`report_gpu()`
:   Check gpu usage and also clean the cache on gpu.

    
`singleton(cls)`
:   A decorator wrapper.
    Used to wrap Llama2Cpp class.
    Make sure there is only one instance running at the time.
    Otherwise the GPU V memory will be drained.
    
    Returns:
        Nah.

Classes
-------

`LLMCore(prompt: llmreflect.Prompt.BasicPrompt.BasicPrompt, llm: langchain.schema.language_model.BaseLanguageModel)`
:   Chain to run queries against LLMs.
    
    Example:
        .. code-block:: python
    
            from langchain import LLMChain, OpenAI, PromptTemplate
            prompt_template = "Tell me a {adjective} joke"
            prompt = PromptTemplate(
                input_variables=["adjective"], template=prompt_template
            )
            llm = LLMChain(llm=OpenAI(), prompt=prompt)
    
    Create a new model by parsing and validating input data from keyword arguments.
    
    Raises ValidationError if the input data cannot be parsed to form a valid model.

    ### Ancestors (in MRO)

    * langchain.chains.llm.LLMChain
    * langchain.chains.base.Chain
    * langchain.load.serializable.Serializable
    * pydantic.main.BaseModel
    * pydantic.utils.Representation
    * langchain.schema.runnable.base.Runnable
    * typing.Generic
    * abc.ABC

    ### Descendants

    * llmreflect.LLMCore.LLMCore.LlamacppCore
    * llmreflect.LLMCore.LLMCore.OpenAICore

    ### Instance variables

    `is_local: bool`
    :   Tell whether a model is local model or openai model.
        Returns:
            bool: If local model then true, otherwise false.

    ### Methods

    `get_inputs(self) ‑> List[str]`
    :   showing inputs for the prompt template being used
        Returns:
            List: a list of strings

`LOCAL_MODEL()`
:   A constant class storing local model path

    ### Class variables

    `base_dir`
    :

    `guanaco_70b_b`
    :

    `llama2_uncensor_70_b`
    :

    `upstage_30_b`
    :

    `upstage_70_b`
    :

`LlamacppCore(model_path: str, prompt_name: str = '', max_total_tokens: int = 2048, max_output_tokens: int = 512, temperature: float = 0.0, verbose: bool = False, n_gpus_layers: int = 8, n_threads: int = 16, n_batch: int = 512)`
:   Chain to run queries against LLMs.
    
    Example:
        .. code-block:: python
    
            from langchain import LLMChain, OpenAI, PromptTemplate
            prompt_template = "Tell me a {adjective} joke"
            prompt = PromptTemplate(
                input_variables=["adjective"], template=prompt_template
            )
            llm = LLMChain(llm=OpenAI(), prompt=prompt)
    
    The LLMCore class for Llamacpp.
    Args:
        model_path (str): Path to the model.
        prompt_name (str, optional): name for prompt. Defaults to ''.
        max_total_tokens (int, optional): Maximum context size.
            Defaults to 2048.
        max_output_tokens (int, optional): Maximum size of completion.
            Defaults to 512.
        temperature (float, optional): Flexibility of the model.
            Defaults to 0.0.
        verbose (bool, optional): whether to print the status.
            Defaults to False.
        n_gpus_layers (int, optional): number of layer to load on gpu.
            Defaults to 8.
        n_threads (int, optional): Number of threads to use.
            Defaults to 16.
        n_batch (int, optional): Maximum number of prompt tokens to batch
            together when calling llama_eval.
            Defaults to 512.

    ### Ancestors (in MRO)

    * llmreflect.LLMCore.LLMCore.LLMCore
    * langchain.chains.llm.LLMChain
    * langchain.chains.base.Chain
    * langchain.load.serializable.Serializable
    * pydantic.main.BaseModel
    * pydantic.utils.Representation
    * langchain.schema.runnable.base.Runnable
    * typing.Generic
    * abc.ABC

    ### Methods

    `generate(self, input_list: List[Dict[str, Any]], run_manager: Optional[langchain.callbacks.manager.CallbackManagerForChainRun] = None) ‑> langchain.schema.output.LLMResult`
    :   The core function for generating LLM result from inputs.
        By using the "check_current_openai_balance". The generation
        will be stopped when the cost is going to exceed the budget.

    `get_inputs(self) ‑> List[str]`
    :   showing inputs for the prompt template being used
        Returns:
            List: A list of input variable, each one should be str

    `predict(self, **kwargs: Any) ‑> str`
    :   The llm prediction interface.
        Returns:
            str: The output / completion generated by llm.

`OPENAI_MODEL()`
:   A constant class storing openai models

    ### Class variables

    `ada`
    :

    `babbage`
    :

    `code_cushman_001`
    :

    `code_cushman_002`
    :

    `code_davinci_001`
    :

    `code_davinci_002`
    :

    `curie`
    :

    `davinci`
    :

    `gpt_3_5_turbo`
    :

    `gpt_3_5_turbo_0301`
    :

    `gpt_3_5_turbo_0613`
    :

    `gpt_3_5_turbo_16k`
    :

    `gpt_3_5_turbo_16k_0613`
    :

    `gpt_4`
    :

    `gpt_4_0314`
    :

    `gpt_4_0613`
    :

    `gpt_4_32k`
    :

    `gpt_4_32k_0314`
    :

    `gpt_4_32k_0613`
    :

    `text_ada_001`
    :

    `text_babbage_001`
    :

    `text_curie_001`
    :

    `text_davinci_002`
    :

    `text_davinci_003`
    :

`OpenAICore(open_ai_key: str, prompt_name: str = '', max_output_tokens: int = 512, temperature: float = 0.0, llm_model='gpt-3.5-turbo')`
:   Chain to run queries against LLMs.
    
    Example:
        .. code-block:: python
    
            from langchain import LLMChain, OpenAI, PromptTemplate
            prompt_template = "Tell me a {adjective} joke"
            prompt = PromptTemplate(
                input_variables=["adjective"], template=prompt_template
            )
            llm = LLMChain(llm=OpenAI(), prompt=prompt)
    
    LLMCore class specifically designed for openAI.
    Args:
        open_ai_key (str): OpenAI key
        prompt_name (str, optional): name for the prompt. Defaults to ''.
        max_output_tokens (int, optional): maximum number of output tokens.
            Defaults to 512.
        temperature (float, optional): Flexibility of the output.
            Defaults to 0.0.
        llm_model (str, optional): string indicating the mode to use.
            Should be included in class LLM_BACKBONE_MODEL.
            Defaults to LLM_BACKBONE_MODEL.gpt_3_5_turbo.

    ### Ancestors (in MRO)

    * llmreflect.LLMCore.LLMCore.LLMCore
    * langchain.chains.llm.LLMChain
    * langchain.chains.base.Chain
    * langchain.load.serializable.Serializable
    * pydantic.main.BaseModel
    * pydantic.utils.Representation
    * langchain.schema.runnable.base.Runnable
    * typing.Generic
    * abc.ABC

    ### Methods

    `generate(self, input_list: List[Dict[str, Any]], run_manager: Optional[langchain.callbacks.manager.CallbackManagerForChainRun] = None) ‑> langchain.schema.output.LLMResult`
    :   The core function for generating LLM result from inputs.
        By using the "check_current_openai_balance". The generation
        will be stopped when the cost is going to exceed the budget.

    `get_inputs(self) ‑> List[str]`
    :   showing inputs for the prompt template being used
        Returns:
            List: A list of input variable, each one should be str

    `predict(self, **kwargs: Any) ‑> str`
    :   The llm prediction interface.
        Returns:
            str: The output / completion generated by llm.